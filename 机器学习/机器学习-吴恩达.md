## 2-1 模型描述
Training set
![[Pasted image 20220121112914.png]]
univarabale 单变量
$(x^{(i)}, y^{(i)})$ 

## 2-2 代价函数
尽量选择参数，使预测的值和实际值接近。
$$minimize\ \frac{1}{2m}\sum_{i=1}^m(h_\theta (x)-y)^2$$ 1/2使为了简化表达式

![[Pasted image 20220121115150.png]]
代价函数是**参数的函数**

代价函数的**等高线**

目标：找到自动寻找cost function J最小值的算法

## 2-5 梯度下降
![[Pasted image 20220121121532.png]]
特点：不同初始点得到不同的局部最小值
算法：
![[Pasted image 20220121121921.png]]
alpha：learning rate（学习率）控制梯度下降的步长，步长的设置影响下降速度和精度

要求：**同步更新**所有参数，先存在缓存中
![[Pasted image 20220121122446.png]]

移动的幅度会随导数值的变换而变化。若在接近最小值的过程中斜率绝对值变小（convex function），则不用减小alpha的大小也会收敛

## 2-7 线性回归的梯度下降
将线性回归的代价函数带入求梯度
$$
\begin{aligned}
\frac{\partial}{\partial \theta_j}J(\theta_0, \theta_1) &= \frac{\partial}{\partial \theta_j}\frac{1}{2m}\sum_{i=1}^m \left(h_\theta (x^{(i)})-y^{(i)}\right)^2 \\
&= \frac{\partial}{\partial \theta_j}\frac{1}{2m}\sum_{i=1}^m(\theta_0 + \theta_1 x^{(i)} - y^{(i)})^2
\end{aligned}
$$
“singular” or “degenerate” 奇异矩阵或退化矩阵，没有逆
transpose：转置

## 4-1 多元
在有n个参数的模型中，用矢量和矩阵来描述代价函数 $h_\theta(x)=\theta_0+\theta_1 x_1 +\cdots + \theta_n x_n$ 。令 $x_0 = 1$ ，则 $h_\theta (x) = \theta^T x$ ，其中，$\theta , x \in \mathbb{R}^{n+1}$ 。
$x^{(i)}\in \mathbb{R}^{n}$ 代表第i个训练样本的输入特征**矢量**。
![[Pasted image 20220121160744.png]]
多元的梯度下降法：
$$
$$

## 4-2 特征缩放
不同变量取值区间差距**过大**时，会使得收敛速度大大降低，最好将所有变量 $x_i$ 的取值缩放到-1到+1之间。
均值归一化：使得训练集的均值为0
![[Pasted image 20220121162551.png]]
归一化不用太严格，目的只是让不同变量取值范围接近，使收敛速度加快。

## 4-3 学习率
- 画J-n图判断是否收敛
- 设置收敛阈值 $\varepsilon$ 判断是否收敛——适当地取值
- J值随n发散——减小 $\alpha$ 
- **当 $\alpha$ 足够小时**，对于线性回归的J总是收敛的
![[Pasted image 20220121163912.png]]

## 4-5 特征和多项式回归
